{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7d32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Neccessary Libraries\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import os, random, time, copy, scipy, pickle, sys, math\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import os.path as path\n",
    "import scipy.io as sio\n",
    "from scipy import misc\n",
    "from scipy import ndimage, signal\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "from io import BytesIO\n",
    "from skimage import data, img_as_float\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import sklearn.metrics \n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler \n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c787d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "# set device, which gpu to use.\n",
    "device ='cpu'\n",
    "if torch.cuda.is_available(): \n",
    "    device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425fee2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd6e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/content/drive/MyDrive/Data_2/CIFAR100_L_T/datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff4e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_working_dir = os.getcwd()\n",
    "project_name = 'demo_1'\n",
    "imb_type = 'exp' # samling long-tailed training set with an exponetially-decaying function\n",
    "imb_factor = 0.01 # imbalance factor = 100 = 1/0.01\n",
    "    \n",
    "nClasses = 100  # number of classes in CIFAR100-LT with imbalance factor 100\n",
    "encoder_num_layers = 13 # network architecture is VGG16\n",
    "batch_size = 64 # batch size \n",
    "isPretrained = True\n",
    "\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.empty_cache()\n",
    "save_dir = path.join(curr_working_dir, 'exp', project_name)\n",
    "if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "\n",
    "    \n",
    "log_filename = os.path.join(save_dir, 'train.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e683a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_DB = '/content/drive/MyDrive/Data_2/CIFAR100_L_T/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a47a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_num_per_cls(cls_num, total_num, imb_type, imb_factor):\n",
    "    # This function is excerpted from a publicly available code [commit 6feb304, MIT License]:\n",
    "    # https://github.com/kaidic/LDAM-DRW/blob/master/imbalance_cifar.py\n",
    "    img_max = total_num / cls_num\n",
    "    img_num_per_cls = []\n",
    "    if imb_type == 'exp':\n",
    "        for cls_idx in range(cls_num):\n",
    "            num = img_max * (imb_factor**(cls_idx / (cls_num - 1.0)))\n",
    "            img_num_per_cls.append(int(num))\n",
    "    elif imb_type == 'step':\n",
    "        for cls_idx in range(cls_num // 2):\n",
    "            img_num_per_cls.append(int(img_max))\n",
    "        for cls_idx in range(cls_num // 2):\n",
    "            img_num_per_cls.append(int(img_max * imb_factor))\n",
    "    else:\n",
    "        img_num_per_cls.extend([int(img_max)] * cls_num)\n",
    "    return img_num_per_cls\n",
    "\n",
    "\n",
    "def gen_imbalanced_data(img_num_per_cls, imgList, labelList):\n",
    "    # This function is excerpted from a publicly available code [commit 6feb304, MIT License]:\n",
    "    # https://github.com/kaidic/LDAM-DRW/blob/master/imbalance_cifar.py\n",
    "    new_data = []\n",
    "    new_targets = []\n",
    "    targets_np = np.array(labelList, dtype=np.int64)\n",
    "    classes = np.unique(targets_np)\n",
    "    # np.random.shuffle(classes)  # remove shuffle in the demo fair comparision\n",
    "    num_per_cls_dict = dict()\n",
    "    for the_class, the_img_num in zip(classes, img_num_per_cls):\n",
    "        num_per_cls_dict[the_class] = the_img_num\n",
    "        idx = np.where(targets_np == the_class)[0]\n",
    "        #np.random.shuffle(idx) # remove shuffle in the demo fair comparision\n",
    "        selec_idx = idx[:the_img_num]\n",
    "        new_data.append(imgList[selec_idx, ...])\n",
    "        new_targets.extend([the_class, ] * the_img_num)\n",
    "    new_data = np.vstack(new_data)\n",
    "    return (new_data, new_targets)\n",
    "\n",
    "\n",
    "\n",
    "class CIFAR100LT(Dataset):\n",
    "    def __init__(self, set_name, imageList=[], labelList=[], labelNames=[], isAugment=True):\n",
    "        self.isAugment = isAugment\n",
    "        self.set_name = set_name\n",
    "        self.labelNames = labelNames\n",
    "        if self.set_name=='train':            \n",
    "            self.transform = transforms.Compose([\n",
    "                # transforms.RandomRotation(45),\n",
    "                transforms.RandomResizedCrop(224),                                                 \n",
    "                # transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),                                 \n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "        \n",
    "        self.imageList = imageList\n",
    "        self.labelList = labelList\n",
    "        self.current_set_len = len(self.labelList)\n",
    "        \n",
    "    def __len__(self):        \n",
    "        return self.current_set_len\n",
    "    \n",
    "    def __getitem__(self, idx):   \n",
    "        curImage = self.imageList[idx]\n",
    "        curLabel =  np.asarray(self.labelList[idx])\n",
    "        curImage = PIL.Image.fromarray(curImage.transpose(1,2,0))\n",
    "        curImage = self.transform(curImage)     \n",
    "        curLabel = torch.from_numpy(curLabel.astype(np.float32)).unsqueeze(0).unsqueeze(0)\n",
    "        return curImage, curLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb10273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unpickling Data Files\n",
    "\n",
    "path_to_DB = path.join(path_to_DB, 'cifar-100-python')\n",
    "\n",
    "datasets = {}\n",
    "dataloaders = {}\n",
    "\n",
    "setname = 'meta'\n",
    "with open(os.path.join(path_to_DB, setname), 'rb') as obj:\n",
    "    labelnames = pickle.load(obj, encoding='bytes')\n",
    "    labelnames = labelnames[b'fine_label_names']\n",
    "for i in range(len(labelnames)):\n",
    "    labelnames[i] = labelnames[i].decode(\"utf-8\") \n",
    "    \n",
    "    \n",
    "setname = 'train'\n",
    "with open(os.path.join(path_to_DB, setname), 'rb') as obj:\n",
    "    DATA = pickle.load(obj, encoding='bytes')\n",
    "new_imgList = DATA[b'data'].reshape((DATA[b'data'].shape[0],3, 32,32))\n",
    "new_labelList = DATA[b'fine_labels']\n",
    "total_num = len(new_labelList)\n",
    "img_num_per_cls = get_img_num_per_cls(nClasses, total_num, imb_type, imb_factor)\n",
    "new_imgList, new_labelList = gen_imbalanced_data(img_num_per_cls, imgList, labelList)\n",
    "datasets[setname] = CIFAR100LT(\n",
    "    imageList=new_imgList, labelList=new_labelList, labelNames=labelnames,\n",
    "    set_name=setname, isAugment=setname=='train')\n",
    "print('#examples in {}-set:'.format(setname), datasets[setname].current_set_len)\n",
    "\n",
    "\n",
    "\n",
    "setname = 'test'\n",
    "with open(os.path.join(path_to_DB, setname), 'rb') as obj:\n",
    "    DATA = pickle.load(obj, encoding='bytes')\n",
    "imgList = DATA[b'data'].reshape((DATA[b'data'].shape[0],3, 32,32))\n",
    "labelList = DATA[b'fine_labels']\n",
    "total_num = len(labelList)\n",
    "datasets[setname] = CIFAR100LT(\n",
    "    imageList=imgList, labelList=labelList, labelNames=labelnames,\n",
    "    set_name=setname, isAugment=setname=='train')\n",
    "\n",
    "\n",
    "print('#examples in {}-set:'.format(setname), datasets[setname].current_set_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fb87f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {set_name: DataLoader(datasets[set_name],\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=set_name=='train', \n",
    "                                    ) # num_work can be set to batch_size\n",
    "               for set_name in ['train', 'test']} # 'train',\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('#train batch:', len(dataloaders['train']), '\\t#test batch:', len(dataloaders['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a17152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[0]*100\n",
    "for f in range(100):\n",
    "    b[f]=f\n",
    "num_of_img=[0]*100\n",
    "for v in range(len(new_labelList)):\n",
    "    for t in range(len(b)):\n",
    "        \n",
    "        if new_labelList[v]==b[t]:\n",
    "            num_of_img[t]+=1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes={'train':50000,\n",
    "               'test':10000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "true_value=[0]*100\n",
    "pred_value=[0]*100\n",
    "\n",
    "# defining train model\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=75):\n",
    "    since = time.time()\n",
    "\n",
    "#     best_model_wts = copy.deepcopy(model.state_dict())\n",
    "#     best_acc = 0.0\n",
    "    n=0\n",
    "    iters=[]\n",
    "    epoch_loss={\"train\":[],\n",
    "                \"test\":[]}\n",
    "    epoch_acc={\"train\":[],\n",
    "               \"test\":[]}\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "            \n",
    "\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                modi_labels=torch.zeros(len(labels))\n",
    "                modi_labels=modi_labels.to(device)\n",
    "              \n",
    "                for h in range(len(labels)):\n",
    "\n",
    "                          modi_labels[h]=labels[h][0,0].int()\n",
    "                print(' ',modi_labels.shape)            \n",
    "                index=[0]*len(labels)\n",
    "                if phase=='test':\n",
    "                    for g in range(len(labels)):                      \n",
    "                            m= modi_labels[g]\n",
    "                            m=m.int()\n",
    "                            true_value[m]+=1                               \n",
    "                            index[g]=m\n",
    "                            index[g]=index[g].int()\n",
    "                          \n",
    "                else:\n",
    "                        pass\n",
    "   \n",
    "\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                   \n",
    "                    outputs = model(inputs) \n",
    "                    outputs=outputs.to(device)\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    preds=preds.to(device)\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "                   \n",
    "                    loss = criterion(outputs, modi_labels.long())\n",
    "                   \n",
    "                        \n",
    "                         \n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        if epoch%20==0 and epoch>30:\n",
    "                          print(len(labels),' ',labels[0],' ',outputs[0],' ',preds[0],' ',len(inputs),' ')\n",
    "                        optimizer.zero_grad()\n",
    "                        \n",
    "                        loss.backward()\n",
    "\n",
    "                            \n",
    "                        optimizer.step()\n",
    "                        scheduler.step()\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                if epoch%30==0:\n",
    "                 print(' ',preds[0],' ',modi_labels[0])\n",
    "                running_corrects += torch.sum(preds == modi_labels)\n",
    "\n",
    "\n",
    "                if phase=='test':\n",
    "                   \n",
    "                  for g in range(len(labels)):\n",
    "                    if preds[g]== modi_labels[g]:\n",
    "                        pred_value[index[g]]=pred_value[index[g]]+1\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "                                     \n",
    "\n",
    "\n",
    "            epoch_loss[phase].append(running_loss / dataset_sizes[phase])\n",
    "            epoch_acc[phase].append(running_corrects.double() / dataset_sizes[phase])\n",
    "\n",
    "            \n",
    "            iters.append(n)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, running_loss / dataset_sizes[phase], running_corrects.double() / dataset_sizes[phase]))\n",
    "            n+=1\n",
    "        if epoch%50==0:     \n",
    "            torch.save(model.state_dict(), 'C:\\\\Users\\\\HP\\\\CIFAR100_L_T\\\\datasets\\\\cifar-100-python\\\\model'+str(epoch)+'.pt')\n",
    "            torch.save(optimizer.state_dict(), 'C:\\\\Users\\\\HP\\\\CIFAR100_L_T\\\\datasets\\\\cifar-100-python\\\\optimizer'+str(epoch)+'.pt')\n",
    "            torch.save(scheduler.state_dict(), 'C:\\\\Users\\\\HP\\\\CIFAR100_L_T\\\\datasets\\\\cifar-100-python\\\\scheduler'+str(epoch)+'.pt')     \n",
    "        iters.append(n)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print()\n",
    "    plt.plot(iters, epoch_acc['train'], label='train')\n",
    "\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Train Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()    \n",
    "\n",
    "    plt.plot(iters, epoch_acc['test'], label='test')\n",
    "\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Test Accuracy\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()    \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82150774",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=models.vgg16(pretrained=True)\n",
    "model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3905b607",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier.append(nn.Sequential(nn.Linear(in_features=1000, out_features=512, bias=True),\n",
    "                                  nn.BatchNorm1d(512),    \n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  \n",
    "    #  nn.Dropout(p=0.5, inplace=False),\n",
    "                                  nn.Linear(in_features=512, out_features=256, bias=True),\n",
    "                                  nn.BatchNorm1d(256),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  nn.Linear(in_features=256, out_features=128, bias=True),\n",
    "                                  nn.BatchNorm1d(128),\n",
    "                                  nn.ReLU(inplace=True),\n",
    "                                  nn.Linear(in_features=128, out_features=100, bias=True)\n",
    "                                  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.005)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 30 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "model=train_model(model,criterion,optimizer,scheduler,num_epochs=150)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
