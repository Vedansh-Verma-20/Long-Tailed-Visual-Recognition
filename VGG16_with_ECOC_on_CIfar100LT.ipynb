{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468f8a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Neccessary libraries and modules\n",
    "\n",
    "from __future__ import print_function, division\n",
    "import os, random, time, copy, scipy, pickle, sys, math\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import os.path as path\n",
    "import scipy.io as sio\n",
    "from scipy import misc\n",
    "from scipy import ndimage, signal\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image\n",
    "from io import BytesIO\n",
    "from skimage import data, img_as_float\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import sklearn.metrics \n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler \n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ad26bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as nnf\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "# set device, which gpu to use.\n",
    "device ='cpu'\n",
    "if torch.cuda.is_available(): \n",
    "    device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41bf368",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_working_dir = os.getcwd()\n",
    "project_name = 'demo_1'\n",
    "imb_type = 'exp' # samling long-tailed training set with an exponetially-decaying function\n",
    "imb_factor = 0.01 # imbalance factor = 100 = 1/0.01\n",
    "    \n",
    "nClasses = 100  # number of classes in CIFAR100-LT with imbalance factor 100\n",
    "encoder_num_layers = 13 # network architecture is VGG16\n",
    "batch_size = 16 # batch size \n",
    "isPretrained = True\n",
    "\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.empty_cache()\n",
    "save_dir = path.join(curr_working_dir, 'exp', project_name)\n",
    "if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "\n",
    "    \n",
    "log_filename = os.path.join(save_dir, 'train.log')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd616b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_DB = './datasets'\n",
    "if not os.path.exists(path_to_DB): os.makedirs(path_to_DB)\n",
    "_ = torchvision.datasets.CIFAR10(root=path_to_DB, train=True, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d9bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_DB='C:\\\\Users\\\\HP\\\\CIFAR100_L_T\\\\datasets'\n",
    "path_to_DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c8e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_num_per_cls(cls_num, total_num, imb_type, imb_factor):\n",
    "    # This function is excerpted from a publicly available code [commit 6feb304, MIT License]:\n",
    "    # https://github.com/kaidic/LDAM-DRW/blob/master/imbalance_cifar.py\n",
    "    img_max = total_num / cls_num\n",
    "    img_num_per_cls = []\n",
    "    if imb_type == 'exp':\n",
    "        for cls_idx in range(cls_num):\n",
    "            num = img_max * (imb_factor**(cls_idx / (cls_num - 1.0)))\n",
    "            img_num_per_cls.append(int(num))\n",
    "    elif imb_type == 'step':\n",
    "        for cls_idx in range(cls_num // 2):\n",
    "            img_num_per_cls.append(int(img_max))\n",
    "        for cls_idx in range(cls_num // 2):\n",
    "            img_num_per_cls.append(int(img_max * imb_factor))\n",
    "    else:\n",
    "        img_num_per_cls.extend([int(img_max)] * cls_num)\n",
    "    return img_num_per_cls\n",
    "\n",
    "\n",
    "def gen_imbalanced_data(img_num_per_cls, imgList, labelList):\n",
    "    # This function is excerpted from a publicly available code [commit 6feb304, MIT License]:\n",
    "    # https://github.com/kaidic/LDAM-DRW/blob/master/imbalance_cifar.py\n",
    "    new_data = []\n",
    "    new_targets = []\n",
    "    targets_np = np.array(labelList, dtype=np.int64)\n",
    "    classes = np.unique(targets_np)\n",
    "    # np.random.shuffle(classes)  # remove shuffle in the demo fair comparision\n",
    "    num_per_cls_dict = dict()\n",
    "    for the_class, the_img_num in zip(classes, img_num_per_cls):\n",
    "        num_per_cls_dict[the_class] = the_img_num\n",
    "        idx = np.where(targets_np == the_class)[0]\n",
    "        #np.random.shuffle(idx) # remove shuffle in the demo fair comparision\n",
    "        selec_idx = idx[:the_img_num]\n",
    "        new_data.append(imgList[selec_idx, ...])\n",
    "        new_targets.extend([the_class, ] * the_img_num)\n",
    "    new_data = np.vstack(new_data)\n",
    "    return (new_data, new_targets)\n",
    "\n",
    "\n",
    "\n",
    "class CIFAR100LT(Dataset):\n",
    "    def __init__(self, set_name='train', imageList=[], labelList=[], labelNames=[], isAugment=True):\n",
    "        self.isAugment = isAugment\n",
    "        self.set_name = set_name\n",
    "        self.labelNames = labelNames\n",
    "        if self.set_name=='train':            \n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomCrop(32, padding=4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "            ])\n",
    "        \n",
    "        self.imageList = imageList\n",
    "        self.labelList = labelList\n",
    "        self.current_set_len = len(self.labelList)\n",
    "        \n",
    "    def __len__(self):        \n",
    "        return self.current_set_len\n",
    "    \n",
    "    def __getitem__(self, idx):   \n",
    "        curImage = self.imageList[idx]\n",
    "        curLabel =  np.asarray(self.labelList[idx])\n",
    "        curImage = PIL.Image.fromarray(curImage.transpose(1,2,0))\n",
    "        curImage = self.transform(curImage)     \n",
    "        curLabel = torch.from_numpy(curLabel.astype(np.float32)).unsqueeze(0).unsqueeze(0)\n",
    "        return curImage, curLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b83a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_DB = path.join(path_to_DB, 'cifar-100-python')\n",
    "## Unpickling data files\n",
    "datasets = {}\n",
    "dataloaders = {}\n",
    "\n",
    "setname = 'meta'\n",
    "with open(os.path.join(path_to_DB, setname), 'rb') as obj:\n",
    "    labelnames = pickle.load(obj, encoding='bytes')\n",
    "    labelnames = labelnames[b'fine_label_names']\n",
    "for i in range(len(labelnames)):\n",
    "    labelnames[i] = labelnames[i].decode(\"utf-8\") \n",
    "    \n",
    "    \n",
    "setname = 'train'\n",
    "with open(os.path.join(path_to_DB, setname), 'rb') as obj:\n",
    "    DATA = pickle.load(obj, encoding='bytes')\n",
    "imgList = DATA[b'data'].reshape((DATA[b'data'].shape[0],3, 32,32))\n",
    "labelList = DATA[b'fine_labels']\n",
    "total_num = len(labelList)\n",
    "img_num_per_cls = get_img_num_per_cls(nClasses, total_num, imb_type, imb_factor)\n",
    "new_imgList, new_labelList = gen_imbalanced_data(img_num_per_cls, imgList, labelList)\n",
    "datasets[setname] = CIFAR100LT(\n",
    "    imageList=new_imgList, labelList=new_labelList, labelNames=labelnames,\n",
    "    set_name=setname, isAugment=setname=='train')\n",
    "print('#examples in {}-set:'.format(setname), datasets[setname].current_set_len)\n",
    "\n",
    "\n",
    "\n",
    "setname = 'test'\n",
    "with open(os.path.join(path_to_DB, setname), 'rb') as obj:\n",
    "    DATA = pickle.load(obj, encoding='bytes')\n",
    "imgList = DATA[b'data'].reshape((DATA[b'data'].shape[0],3, 32,32))\n",
    "labelList = DATA[b'fine_labels']\n",
    "total_num = len(labelList)\n",
    "datasets[setname] = CIFAR100LT(\n",
    "    imageList=imgList, labelList=labelList, labelNames=labelnames,\n",
    "    set_name=setname, isAugment=setname=='train')\n",
    "\n",
    "\n",
    "print('#examples in {}-set:'.format(setname), datasets[setname].current_set_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a284401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preview img_num_per_cls distribution\n",
    "plt.plot(img_num_per_cls)\n",
    "plt.xlabel('class ID sorted by cardinality')\n",
    "plt.ylabel('#training examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2272cf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = {set_name: DataLoader(datasets[set_name],\n",
    "                                    batch_size=batch_size,\n",
    "                                    shuffle=set_name=='train', \n",
    "                                    ) # num_work can be set to batch_size\n",
    "               for set_name in ['train', 'test']} # 'train',\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('#train batch:', len(dataloaders['train']), '\\t#test batch:', len(dataloaders['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e78a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[0]*100\n",
    "for f in range(100):\n",
    "    b[f]=f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding number of images\n",
    "num_of_img=[0]*100\n",
    "for v in range(len(new_labelList)):\n",
    "    for t in range(len(b)):\n",
    "        \n",
    "        if new_labelList[v]==b[t]:\n",
    "            num_of_img[t]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f0f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding tail classes\n",
    "tail_classes=[]\n",
    "for t in range(len(b)):\n",
    "    if num_of_img[t]<10:\n",
    "        tail_classes.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f2653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preview training data representation\n",
    "plt.plot(num_of_img)\n",
    "plt.xlabel('class ID sorted by cardinality')\n",
    "plt.ylabel(' No. of training examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73749ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sizes={'train':10847,\n",
    "               'test':10000}\n",
    "traindata_labels=new_labelList\n",
    "testdata_labels=labelList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0bfdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_m=torch.randint(0,2,(100,10)).unique\n",
    "# Creating a gold matrix(actually a code matrix) using output of code_m\n",
    "gold_matrix=[[1, 1, 0, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "        [1, 0, 1, 0, 1, 1, 0, 1, 1, 0],\n",
    "        [1, 0, 0, 0, 1, 1, 0, 0, 1, 0],\n",
    "        [0, 1, 0, 1, 1, 0, 0, 1, 1, 1],\n",
    "        [0, 0, 1, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 1, 1, 1, 1, 0, 0, 1, 0, 0],\n",
    "        [1, 0, 0, 0, 1, 0, 1, 1, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 1, 1, 1, 0],\n",
    "        [1, 1, 0, 1, 0, 0, 0, 0, 1, 1],\n",
    "        [1, 1, 1, 0, 1, 0, 1, 1, 1, 1],\n",
    "        [1, 0, 0, 1, 1, 0, 0, 1, 1, 1],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 0, 1, 0, 1],\n",
    "        [1, 0, 0, 1, 1, 0, 1, 0, 0, 1],\n",
    "        [1, 0, 0, 1, 0, 0, 0, 1, 1, 1],\n",
    "        [0, 1, 1, 0, 0, 1, 1, 1, 1, 0],\n",
    "        [1, 1, 0, 0, 1, 1, 1, 1, 1, 0],\n",
    "        [0, 1, 0, 1, 0, 1, 1, 0, 1, 1],\n",
    "        [1, 1, 1, 1, 0, 0, 1, 1, 0, 0],\n",
    "        [0, 0, 1, 1, 0, 0, 0, 0, 1, 0],\n",
    "        [0, 0, 1, 1, 1, 0, 1, 1, 0, 1],\n",
    "        [0, 1, 1, 0, 0, 1, 1, 0, 1, 0],\n",
    "        [0, 0, 1, 1, 1, 1, 1, 0, 1, 0],\n",
    "        [0, 1, 0, 1, 0, 0, 0, 1, 1, 1],\n",
    "        [1, 0, 1, 1, 0, 0, 1, 0, 1, 0],\n",
    "        [1, 0, 1, 0, 1, 1, 1, 1, 1, 1],\n",
    "        [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "        [0, 0, 0, 1, 0, 1, 1, 1, 1, 0],\n",
    "        [1, 0, 1, 0, 1, 1, 0, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 0, 1, 0, 1, 0],\n",
    "        [0, 0, 1, 1, 0, 1, 1, 0, 1, 0],\n",
    "        [0, 1, 0, 1, 1, 1, 1, 0, 1, 1],\n",
    "        [1, 0, 0, 1, 1, 1, 1, 1, 0, 1],\n",
    "        [0, 0, 1, 0, 1, 0, 0, 1, 0, 1],\n",
    "        [1, 1, 1, 1, 1, 0, 0, 1, 0, 0],\n",
    "        [1, 0, 1, 1, 0, 1, 1, 1, 1, 0],\n",
    "        [1, 0, 1, 0, 0, 1, 0, 1, 1, 0],\n",
    "        [0, 0, 0, 1, 0, 1, 0, 0, 1, 0],\n",
    "        [1, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [0, 1, 0, 0, 1, 0, 0, 1, 0, 1],\n",
    "        [0, 0, 1, 0, 0, 1, 1, 0, 0, 1],\n",
    "        [0, 1, 1, 0, 1, 1, 0, 1, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0, 1, 1, 0, 1],\n",
    "        [1, 1, 0, 1, 1, 1, 1, 1, 0, 1],\n",
    "        [1, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
    "        [0, 1, 0, 0, 0, 1, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 1],\n",
    "        [0, 1, 0, 0, 1, 0, 1, 1, 1, 1],\n",
    "        [1, 1, 0, 1, 0, 0, 1, 1, 1, 0],\n",
    "        [1, 1, 0, 1, 0, 1, 0, 1, 0, 1],\n",
    "        [1, 0, 1, 1, 0, 0, 1, 0, 1, 1],\n",
    "        [0, 0, 1, 0, 1, 0, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 0, 0, 0, 1, 0, 0, 0],\n",
    "        [0, 1, 1, 0, 1, 1, 1, 1, 1, 0],\n",
    "        [1, 0, 1, 0, 0, 1, 0, 1, 0, 0],\n",
    "        [1, 0, 1, 0, 1, 0, 0, 1, 1, 0],\n",
    "        [1, 0, 0, 0, 1, 1, 1, 0, 0, 0],\n",
    "        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "        [0, 1, 1, 1, 0, 0, 1, 0, 0, 0],\n",
    "        [1, 0, 1, 0, 1, 0, 0, 1, 1, 1],\n",
    "        [0, 1, 0, 1, 0, 0, 0, 1, 0, 1],\n",
    "        [0, 0, 0, 0, 1, 0, 0, 1, 0, 1],\n",
    "        [1, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 0, 1, 0, 0, 1, 1, 0, 0, 1],\n",
    "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 0, 0, 1, 0, 0, 0],\n",
    "        [0, 1, 1, 1, 1, 0, 1, 0, 1, 0],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [1, 1, 1, 1, 1, 0, 1, 1, 0, 1],\n",
    "        [1, 0, 1, 1, 0, 0, 0, 0, 1, 1],\n",
    "        [1, 0, 1, 0, 1, 0, 1, 1, 0, 0],\n",
    "        [0, 1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
    "        [1, 1, 0, 1, 0, 1, 1, 0, 1, 0],\n",
    "        [0, 1, 1, 0, 1, 1, 1, 0, 1, 1],\n",
    "        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 0, 1, 1, 1, 0],\n",
    "        [1, 1, 0, 1, 1, 1, 1, 0, 1, 0],\n",
    "        [1, 1, 1, 1, 1, 0, 0, 0, 1, 0],\n",
    "        [1, 0, 0, 0, 1, 0, 1, 0, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 1],\n",
    "        [0, 0, 1, 0, 1, 1, 0, 1, 0, 0],\n",
    "        [0, 0, 1, 1, 1, 0, 1, 0, 0, 0],\n",
    "        [0, 0, 0, 1, 1, 1, 0, 0, 1, 1],\n",
    "        [1, 0, 1, 1, 0, 0, 1, 1, 0, 0],\n",
    "        [0, 0, 1, 1, 1, 1, 1, 0, 0, 1],\n",
    "        [0, 1, 1, 1, 1, 0, 1, 1, 1, 0],\n",
    "        [1, 0, 0, 1, 0, 0, 1, 1, 0, 1],\n",
    "        [0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
    "        [1, 0, 1, 0, 1, 0, 1, 1, 0, 0],\n",
    "        [1, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "        [1, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
    "        [1, 1, 0, 1, 0, 1, 0, 0, 1, 0],\n",
    "        [1, 0, 1, 1, 1, 1, 1, 0, 0, 1],\n",
    "        [0, 0, 1, 1, 1, 1, 0, 0, 1, 0],\n",
    "        [0, 1, 0, 0, 1, 0, 0, 0, 1, 1],\n",
    "        [0, 1, 0, 1, 0, 1, 0, 0, 1, 0],\n",
    "        [0, 1, 1, 0, 0, 0, 1, 1, 0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5aa57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_matrix=torch.tensor(gold_matrix)\n",
    "from tqdm import tqdm\n",
    "# Defining train model\n",
    "\n",
    "true_value=[0]*100\n",
    "pred_value=[0]*100\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=8):\n",
    "    since = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                ## defining modi_label according to the batch_size(or len of labels)\n",
    "                modi_labels=torch.zeros(len(labels))\n",
    "                \n",
    "\n",
    "                index=[0]*len(labels)    \n",
    "                for h in range(len(labels)):\n",
    "\n",
    "                            modi_labels[h]=labels[h][0,0].int()\n",
    "              \n",
    "                if phase=='test':  \n",
    "                      for g in range(len(labels)):  \n",
    "                            m=modi_labels[g]\n",
    "\n",
    "                            m=m.int()\n",
    "                            true_value[m]+=1\n",
    "                            index[g]=m\n",
    "\n",
    "                  \n",
    "                else:\n",
    "                        pass\n",
    "   \n",
    "                outputs = model(inputs)\n",
    "                m=nn.Softmax(dim=1)\n",
    "                out=torch.zeros(len(labels),10)\n",
    "\n",
    "\n",
    "                out=m(outputs)\n",
    "\n",
    "\n",
    "\n",
    "                print(' ',modi_labels.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                preds=torch.zeros(len(labels))\n",
    "                preds=preds.int()                    \n",
    "\n",
    "                for s in range(len(labels)):\n",
    "\n",
    "                    for a in range(100):\n",
    "                        # finding dot product of each combination of a out array with each row of gold_matrix and max dot product value chosen as closest codeword\n",
    "\n",
    "                        max=0\n",
    "                        dot_product=np.dot(out[s].detach().numpy(),gold_matrix[a].detach().numpy())\n",
    "                        if dot_product>max:\n",
    "                           max=dot_product\n",
    "                           preds[s]=a                    \n",
    "    \n",
    "                \n",
    "\n",
    "                new_label=torch.zeros(len(labels))\n",
    "\n",
    "                target_code=torch.zeros(len(labels),10)\n",
    "\n",
    "\n",
    "                print(preds)\n",
    "\n",
    "                new_label=modi_labels.long()\n",
    "                for g in range(len(labels)):\n",
    "                       l=new_label[g] \n",
    "                       target_code[g]=gold_matrix[l]\n",
    "    \n",
    "\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled( phase=='train'):\n",
    "                   \n",
    "#                       outputs = model(inputs)\n",
    "                      \n",
    "                  \n",
    "                     \n",
    "\n",
    "                                            \n",
    "                      \n",
    "                      \n",
    "                      \n",
    "                      loss = criterion(out\n",
    "                                       ,target_code)\n",
    "                      \n",
    "                   \n",
    "                      \n",
    "\n",
    "\n",
    "                         \n",
    "\n",
    "                      # backward + optimize only if in training phase\n",
    "                      if phase == 'train':\n",
    "                       \n",
    "                        print(len(labels),' ',labels[0],' ',out[0],' ',preds[0],' ',len(inputs),' ',target_code[0])\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        loss.backward()\n",
    "\n",
    "\n",
    "                        optimizer.step()\n",
    "                print (' ',len(preds),' ',len(modi_labels))\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                running_corrects += torch.sum(preds == modi_labels)\n",
    "\n",
    "\n",
    "                if phase=='test':\n",
    "                   \n",
    "                  for g in range(len(labels)):\n",
    "                    if preds[g]== modi_labels[g]:\n",
    "                        pred_value[index[g]]=pred_value[index[g]]+1\n",
    "                    else:\n",
    "                        pass\n",
    "            if phase == 'train':\n",
    "                 scheduler.step()                    \n",
    "\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "             \n",
    "        torch.save(model.state_dict(), 'C:\\\\Users\\HP\\\\CIFAR100_L_T\\\\datasets\\\\cifar-100-python\\\\model'+str(epoch)+'.pt')\n",
    "        torch.save(optimizer.state_dict(), 'C:\\\\Users\\HP\\\\CIFAR100_L_T\\\\datasets\\\\cifar-100-python\\\\optimizer'+str(epoch)+'.pt')\n",
    "        torch.save(scheduler.state_dict(), 'C:\\\\Users\\HP\\\\CIFAR100_L_T\\\\datasets\\\\cifar-100-python\\\\scheduler'+str(epoch)+'.pt')     \n",
    "\n",
    "\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd700c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Freezing the pre-trained model layer\n",
    "model=models.vgg16(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "##Fine Tuning Conv Net \n",
    "model.classifier=nn.Linear(25088,10)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.classifier.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "model=train_model(model,criterion,optimizer,scheduler,num_epochs=8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
